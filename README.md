# Final-Project-Transforming-and-Analyzing-Data-with-SQL

## Project/Goals
There were several goals within this project related to learning how to set up, clean, and analyze a database via SQL, while also transferring information from source code to GitHub. The project further required learning how to understand different connections across datasets, pull information to answer questions, as well as create queries to explore the data further. Once the data was cleaned and analyzed, we conducted a QA walkthrough of the database to identify and mitigate risks within the data and think through the entire process of data cleaning, transformation, and analysis.

My personal goals for this project were to develop a better working understanding of how to use SQL and pgAdmin. However, I also feel that I gained valuable experience in troubleshooting when encountering issues with query writing. Specifically, I improved my ability to identify errors and debug my written code. I would hope this is transferable into learning other coding languages and applications such as Python, mySQL, or Java.

## Process
The first step in this project was to create five tables and upload the data from the CSV documents. Once completed, I moved on to the data cleaning process. When cleaning the database, I checked for a range of potential issues, including outliers, missing and incomplete data, duplicated entries, incorrect data types, and inconsistent formatting or naming conventions.
 
To address outliers, I started by executing statistical variance queries up to three degrees and imputing null for any missing values. Two outliers were found under the visit ID variable in the analytics table. Additionally, 13,308 data entries were found to be outliers under the unit price variable. These outliers were noted so they could be referred back to after searching for duplication.
 
It is important to note that although the questions under the "starting_with_questions" section suggest that the analysis seeks market information, I personally did not have specific goals for the analysis. Therefore, I conducted a primary check for outliers before removing duplicates, with the understanding that after removing duplicates, it may be necessary to re-check for outliers. This is because the statistical properties of the dataset after duplicate removal can change.
 
Furthermore, on an initial review of the database, the "sales_report" and "product" dataset variables appeared to be duplicated. To investigate, I executed a JOIN query between those two tables, which revealed that all variable names, except for an extra ratio column in the sales table, were a match. The top row values also appeared to be the same. Therefore, to verify if the data was also duplicated, I conducted a further EXCEPT query to review duplication across the tables. This revealed that there were 1009 entries in the "products" table that were not in the "sales_report" table, as well as 371 entries in the "sales_report" table that were not in the "products" table. I made a note to join these tables once further cleaning had been conducted. However, I continued to run a statistical variance check for outliers on the "sales_report" table, which did not return any further outliers.
 
The next step in my cleaning process was to remove non-significant variables with empty rows that could not be calculated using available information. The variables "total_transaction_revenue", "transactions", "session_quality_dim", "product_refund_amount", "product_quantity", "product_revenue", "item_revenue", "item_quantity", "transaction_revenue", "transaction_id", "search_keyword", and "ecommerce_action_option" from the "all_sessions" table appeared to have returned NULL values for all of their rows. A similar result was found in "units_sold", "revenue", "user_id", and "time_on_site" variables from the "analytics" table. Therefore, I used a COUNT DISTINCT query to see if there were unique results for each variable. Five of the variables from the "all_sessions" table showed all NULL values and were subsequently dropped using a DROP COLUMN command. For all of the other variables that had more than one distinct value (NULL), I ran a COUNT IF NULL query to see how many of the total values were NULL. All of the remaining variables except the "time_on_site" variable from the "analytics" table had more than 95% NULL values; therefore, they were also removed using a DROP COLUMN command.
 
The next step in the cleaning process is to remove all duplicates across each dataset. I removed duplicates from within tables, using three commands: CREATE TABLE with SELECT DISTINCT, DROP TABLE, and ALTER TABLE. I further compared data across tables, where there were similar variable names that were not associated with the primary key (set as the SKU during table creation)chat.

Having noted earlier that the sales_report and products tables held the same variables but contained some different values, I chose to join the like variables together using a CREATE TABLE and UNION command. Since the products table did not have a ratio variable, I needed to add one to the products table first so it could be a complete match for the sales_report table. Afterwards I removed the initial products table and the sales_report, before running the same duplicate removal SELECT DISTINCT command, as previously done above. At that time renaming the table to product_sales_report.



## Results
(fill in what you discovered this data could tell you and how you used the data to answer those questions)

## Challenges 
I faced a few challenges during this project. Firstly, while I had a general idea of how I wanted to clean the database and what queries I wanted to run, I struggled with executing them correctly using SQL and pgAdmin. I had a fair amount of trial and error trying to format using correct sytax and proper ordering, and find the correct commands. Stack Overflow and Postgresql Tutorial, alongside other information from the weeks lessons was very helpful. 

Another challenge was that many of the variables across the datasets had similar names.  While some were the same variable and needed to be cleaned, others such as product_price and unit_price were confusing.  For certain questions, I had to make assumptions and then justify my use of a variable that made the most sense to generate a query. One example would be the inclusion of both product_price and unit_price in the products and analytics datasets. I determined that product_price would be the direct amount a customer would pay and would account for other costs such as discounts or shipping, whereas the unit price from analytics would be just the base price of the physical product without anything else considered. Therefore, I used product price to generate revenue. However, it is possible that without accurate information or a descripton of each variable,  even correct queries could retrieve incorrect data.

Finally, I had to prioritize completing each task to a sufficient level, and therefore I did not have enough time to conduct as in-depth data cleaning and analysis as I would have liked. Although I was not able to clean the database as thoroughly as I would have preferred, I documented the data cleaning steps that I had completed and identified what additional work I would have liked to do. If this had been a team project or a professional work environment where I had been reassigned, I would have hoped that my documentation provided enough detail for someone else to pick up where I left off.


## Future Goals
With more time I would have conducted a more throrough cleaning and analysis of the database. Although, I was able to remove exact duplicates from the table, there is still other data that could have been sythesized. 